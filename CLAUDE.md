# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

SEAM (Semantically Equivalent Across Modalities) is a high-performance Vision-Language Model evaluation benchmark that tests models across 4 domains (Chess, Chemistry, Music, Graph Theory) with 16 tasks and 3 modalities (Language-only, Vision-only, Vision-Language). The benchmark comprises 3,200 four-way multiple-choice questions in total.

## Key Architecture

### Core Components
- **`code/utils/task_loader.py`**: Unified task loader supporting both HuggingFace parquet datasets and JSONL files with automatic fallback
- **`code/utils/utils.py`**: Image processing utilities and helper functions  
- **`code/config/config.py`**: Unified configuration file with data source switching and settings for all providers
- **`code/dataset/hf_dataset.py`**: HuggingFace dataset management and upload utilities

### Unified 3-Stage Pipeline
The benchmark uses a clean 3-stage pipeline for all model types:

1. **Stage 1: Inference** - Pure inference with unified output format
2. **Stage 2: Extraction** - Unified answer extraction for all models
3. **Stage 3: Metrics** - Comprehensive metrics and comparison analysis

### Entry Points
- **`code/run/01_inference_vllm.py`**: vLLM inference for local models (with resume support)
- **`code/run/01_inference_openai.py`**: OpenAI inference (real-time + batch API)
- **`code/run/01_inference_claude.py`**: Claude inference (real-time API)
- **`code/run/02_extract.py`**: Unified answer extraction across all model types
- **`code/run/03_metric.py`**: Unified metrics computation and comparison plots

### Data Sources
The benchmark supports two data sources with seamless switching:

#### HuggingFace Dataset (Default)
- **Repository**: `lilvjosephtang/SEAM-Benchmark` 
- **Format**: 16 task-based parquet splits with integrated PIL images
- **Total**: 3,200 base samples (200 per task) 
- **Configuration**: `USE_HF_DATASET = True` in `config.py`
- **Automatic**: Downloads and caches data in `data/` directory

#### JSONL Files (Fallback)
- **Location**: `raw_data/` directory with task-specific subdirectories
- **Format**: JSONL files + PNG images in task folders
- **Configuration**: `USE_HF_DATASET = False` in `config.py`
- **Manual**: Generated by dataset scripts or downloaded separately

**Key Feature**: Both data sources produce 100% identical evaluation results with automatic fallback.

### Data Generation
- **`code/dataset/`**: Contains dataset generation scripts for each domain
  - `dataset_chess.py`, `dataset_chem.py`, `dataset_music.py`, `dataset_graph.py`
  - `hf_dataset.py`: HuggingFace dataset upload and management
  - `graph_tasks/`: Graph theory task implementations

## Common Development Commands

### Stage 1: Inference

```bash
# vLLM (Local Models)
cd code/run && python 01_inference_vllm.py --model Qwen/Qwen2.5-VL-7B-Instruct --modes l,v,vl

# vLLM with custom settings
cd code/run && python 01_inference_vllm.py --model Qwen/Qwen2.5-VL-7B-Instruct \
                                          --tasks fork,legal,puzzle \
                                          --gpu-memory-utilization 0.6 \
                                          --tensor-parallel-size 1

# OpenAI Real-time
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --modes l,v,vl

# OpenAI Batch Processing
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --batch --action all --modes l,v,vl

# OpenAI Batch Step-by-step
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --batch --action prepare --modes l,v,vl
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --batch --action submit
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --batch --action status
cd code/run && python 01_inference_openai.py --model gpt-4o-mini --batch --action download

# Claude Real-time (with parallel processing)
cd code/run && python 01_inference_claude.py --model claude-3-5-sonnet-20241022 --modes l,v,vl

# Claude with custom parallel settings
cd code/run && python 01_inference_claude.py --model claude-3-5-sonnet-20241022 \
                                             --modes l,v,vl \
                                             --max-workers 5

# Claude sequential processing (no parallel)
cd code/run && python 01_inference_claude.py --model claude-3-5-haiku-20241022 \
                                             --modes l \
                                             --no-parallel

# Debug mode with limited samples
cd code/run && python 01_inference_vllm.py --model Qwen/Qwen2.5-VL-7B-Instruct --debug-samples 10 --modes l
```

### Stage 2: Answer Extraction

```bash
# Extract for specific model
cd code/run && python 02_extract.py --model qwen-qwen2.5-vl-7b-instruct

# Extract for all models
cd code/run && python 02_extract.py --all

# Force re-extraction
cd code/run && python 02_extract.py --model gpt-4o-mini --force

# Use custom extraction model
cd code/run && python 02_extract.py --model claude-3-5-sonnet-20241022 --extraction-model Qwen/Qwen2.5-1.5B-Instruct

# List available models
cd code/run && python 02_extract.py --list
```

### Stage 3: Metrics & Analysis

```bash
# Compute metrics for specific model
cd code/run && python 03_metric.py --model qwen-qwen2.5-vl-7b-instruct

# Compute for all models
cd code/run && python 03_metric.py --all

# Generate comparison plots
cd code/run && python 03_metric.py --compare --models qwen-qwen2.5-vl-7b-instruct,gpt-4o-mini,claude-3-5-sonnet-20241022

# List available models with metrics
cd code/run && python 03_metric.py --list
```


### Multi-Provider Model Automation

```bash
# Run complete pipeline for all enabled models with automatic provider detection
./run_all_models.sh

# Edit the script to enable/disable models:
# - vLLM models: Qwen/Qwen2.5-VL-7B-Instruct, OpenGVLab/InternVL3-8B, etc.
# - OpenAI models: gpt-4o-mini, gpt-4o, etc.
# - Claude models: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, etc.

# The script automatically:
# - Detects model type and uses appropriate inference script
# - Runs full 3-stage pipeline: Inference -> Extraction -> Metrics
# - Provides comprehensive logging for each stage
# - Handles errors gracefully and reports final status
```

### Dataset Generation

```bash
# Generate all benchmark datasets
cd code/dataset/
python dataset_chess.py
python dataset_chem.py
python dataset_music.py
python dataset_graph.py
```

### Environment Setup

```bash
# Install dependencies and setup API keys
cd code/config && ./setup.sh

# Or manually:
cd code/config && pip install -r requirements.txt
cp api_keys.json.template api_keys.json
# Edit api_keys.json and add your API keys
```

### API Configuration

The project uses a secure JSON-based API key management system:

1. **Copy template**: `cp api_keys.json.template api_keys.json`
2. **Configure API keys**:
   ```json
   {
     "openai": {
       "api_key": "sk-your-openai-key-here"
     },
     "anthropic": {
       "api_key": "your-anthropic-key-here"
     },
     "huggingface": {
       "api_key": "your-huggingface-token-here"
     }
   }
   ```
3. **Security**: `api_keys.json` is excluded from git via `.gitignore`

**Alternative**: Environment variables are still supported:
```bash
export OPENAI_API_KEY=your_openai_key
export ANTHROPIC_API_KEY=your_anthropic_key
export HF_TOKEN=your_huggingface_token
```

## vLLM Configuration

The pipeline uses vLLM for efficient local model inference:

### GPU Settings
- **GPU Memory Utilization**: 80% by default (configurable with --gpu-memory-utilization)
- **Tensor Parallelism**: 2 GPUs by default (configurable with --tensor-parallel-size)
- **Pipeline Parallelism**: Single GPU by default
- **Max Model Length**: 16384 tokens by default

### Model Settings
- **Sampling**: Uses model defaults (temperature, top_p, etc.) except max_tokens limit
- **Max Tokens**: 8192 for generation, 5 for answer extraction
- **Extraction**: temperature=0.0, max_tokens=5 for strict answer extraction

### Model Compatibility
- **InternVL Support**: Automatic detection of InternVL models with string-based chat template format
- **Unified Vision Handling**: Standard list format for Qwen, LLaVA, and other models
- **Warning Suppression**: Filtered tokenizer warnings for cleaner log output
- **Chat Template Detection**: Automatic fallback to raw prompts for models without chat templates

## Key Configuration

### Unified Configuration (code/config/config.py)
- **GPU Memory Utilization**: 80% (optimized for stability)
- **Tensor Parallel Size**: 2 GPUs (default)
- **Max Model Length**: 16384 tokens
- **Model Temperature**: 0.7 for evaluation, 0.0 for extraction
- **Max Tokens**: 8192 for inference, 5 for extraction
- **Extraction Model**: Qwen/Qwen2.5-7B-Instruct with LLM + regex fallback

### Task Structure
- **16 Tasks**: fork, legal, puzzle, eval (chess); carbon, hydrogen, weight, caption (chemistry); notes, measures, forms, rhythm (music); path_counting, path_existence, shortest_path, bfs_traversal (graph)
- **3 Modalities**: l (language-only), v (vision-only), vl (vision-language)
- **Answer Format**: Multiple choice A/B/C/D with Z for invalid/failed extractions
- **Total Scale**: 9,600 samples (16 tasks × 200 samples × 3 modalities)

## Result Management

### Unified Results Structure
```
results/
├── {model-name}/              # e.g., qwen-qwen2.5-vl-7b-instruct/
│   ├── output.jsonl          # Raw inference outputs (Stage 1)
│   ├── extracted.jsonl       # With extracted answers (Stage 2)
│   ├── metrics.json          # Computed metrics (Stage 3)
│   ├── batch/                # Batch processing files (OpenAI)
│   └── plots/                # Model-specific visualizations
└── comparison_plots.png       # Multi-model comparison
```

### Resume Functionality
- **Automatic Resume**: All inference scripts skip completed samples by default
- **Robust Identification**: Uses task_name + mode + index for reliable resume
- **Error Recovery**: Continue from interruptions without data loss
- **Force Options**: `--no-resume`, `--force` flags for complete re-runs

### Output Formats
- **Unified JSONL**: Consistent schema across all providers with metadata
- **Comprehensive Metrics**: Accuracy by mode/task/domain + performance stats
- **Comparison Plots**: Multi-model visualizations with matplotlib/seaborn
- **Extraction Analytics**: Method tracking (LLM vs regex) and confidence scores

## Unified Answer Extraction System

### Two-Stage Extraction with Confidence
1. **LLM Extraction**: Primary method using Qwen2.5-7B-Instruct (temperature=0, max_tokens=5)
2. **Regex Fallback**: Pattern matching when LLM extraction fails
3. **Confidence Scoring**: Tracks extraction method and confidence level

### Extraction Patterns
- "The best option is A"
- "Answer: B" 
- "Final answer: C"
- Single letter at end of response
- "([A-D])\)?\.*\s*$" and other regex patterns

### Unified Processing
- **Model Agnostic**: Same extraction logic for vLLM, OpenAI, Claude outputs
- **Method Tracking**: Records whether LLM or regex was used
- **Error Handling**: Graceful fallback to "Z" for failed extractions
- **Performance**: Efficient batch processing with progress tracking

## Sampling Configuration

### Main Model Inference
- **Uses model defaults** for temperature, top_p, top_k, and other sampling parameters
- **Only constraint**: max_tokens=8192 to limit response length
- **Rationale**: Allows each model to use its optimal default settings

### Answer Extraction
- **Strict parameters**: temperature=0.0, max_tokens=5
- **Deterministic**: Ensures consistent answer extraction across runs
- **Model**: Qwen2.5-7B-Instruct (configurable via DEFAULT_EXTRACTION_MODEL)

## Performance Considerations

### Inference Optimization
- **vLLM Batching**: Efficient batch processing for local models
- **GPU Memory Management**: Configurable utilization (80% default) with multi-GPU support
- **API Rate Limiting**: Built-in handling for OpenAI and Claude API limits with exponential backoff
- **Parallel Processing**: Claude inference supports ThreadPoolExecutor with configurable workers (default: 10)
- **Resume Support**: Skip completed work to optimize re-runs
- **Multi-round Retry**: Claude inference includes resilient retry system ensuring all prompts are processed

### Processing Efficiency
- **Incremental Saving**: Results saved continuously to prevent data loss
- **Progress Tracking**: Real-time progress bars with throughput metrics
- **Memory Efficiency**: Streaming JSONL processing and batch chunking
- **Error Resilience**: Individual sample failures don't stop entire evaluation

### Scalability Features
- **Provider Abstraction**: Easy addition of new model providers
- **Modular Pipeline**: Independent stages allow selective re-processing
- **Comparison Tools**: Efficient multi-model analysis and visualization
- **Configuration Driven**: Centralized settings for easy parameter tuning

## Troubleshooting

### GPU/Memory Issues
- Reduce `--gpu-memory-utilization` if encountering OOM errors
- Decrease `--max-model-len` for models with limited VRAM
- Monitor GPU usage: `nvidia-smi` or `watch -n1 nvidia-smi`

### Model Loading Issues
- Ensure model is available locally or can be downloaded
- Check HuggingFace cache space: `~/.cache/huggingface/`
- Verify model compatibility with vLLM

### Extraction Issues
- Answer extraction is now a separate stage using `02_extract.py`
- Uses Qwen2.5-7B-Instruct with strict parameters: temperature=0.0, max_tokens=5
- Automatic fallback to regex extraction for failed LLM extractions
- Run extraction manually: `cd code/run && python 02_extract.py --model <model-name>`
- Force re-extraction: `cd code/run && python 02_extract.py --model <model-name> --force`
- List models needing extraction: `cd code/run && python 02_extract.py --list`

### Claude API Issues
- **Rate Limiting**: Built-in exponential backoff retry with up to 20 attempts
- **Connection Errors**: Automatic retry with increasing delays (max 120 seconds)
- **Parallel Processing**: Reduce `--max-workers` if experiencing too many API errors
- **Failed Prompts**: Multi-round retry system ensures all prompts are eventually processed
- **Sequential Mode**: Use `--no-parallel` for more stable processing with rate-limited APIs

### InternVL Chat Template Issues
- **Automatic Detection**: Pipeline automatically detects InternVL models and uses string format
- **Vision Mode Compatibility**: InternVL uses `<image>\n{prompt}` format vs. list format for other models
- **Language Mode**: Works normally with standard chat template for all models
- **No Manual Configuration**: Detection and formatting is handled automatically

### HuggingFace Gated Model Issues
- **Authentication Required**: Models like Llama require HuggingFace tokens for access
- **Token Configuration**: Add `huggingface.api_key` to `api_keys.json` or set `HF_TOKEN` environment variable
- **Automatic Setup**: vLLM script automatically loads and configures HF authentication
- **Error Indication**: 403 authentication errors indicate missing or invalid HF token